

#################
#### MODULES ####

##get_ipython().magic('matplotlib inline')
##get_ipython().magic("config InlineBackend.figure_format = 'retina'")
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import seaborn as sns
import warnings
from pathlib import Path

warnings.filterwarnings('ignore')
pd.options.display.float_format = '{:,.2f}'.format
pd.set_option('display.max_rows', 100)
pd.set_option('display.max_columns', 200)
pd.set_option('display.width', 250)

from __future__ import print_function
from datetime import datetime
from matplotlib.colors import ListedColormap
from sklearn.datasets import make_classification, make_moons, make_circles
from sklearn.metrics import confusion_matrix, classification_report, mean_squared_error, mean_absolute_error, r2_score
from sklearn.linear_model import LogisticRegression
from sklearn.utils import shuffle
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Activation
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.python.keras.utils.np_utils import to_categorical
from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder, MinMaxScaler
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, KFold
import tensorflow.keras.backend as K
from tensorflow.keras.wrappers.scikit_learn import KerasClassifier

import os
os.chdir("/cluster/home/quever/git/Applied-Deep-Learning-with-Keras/py")
from models import *
from P2_utility_functions import *

#######################
#### Visualization ####
def plot_loss_accuracy(hist, outfile):
    acc = hist.history['accuracy']
    val_acc = hist.history['val_accuracy']
    loss = hist.history['loss']
    val_loss = hist.history['val_loss']
    
    fig, (ax1, ax2) = plt.subplots(2, sharex=True)
    ax1.plot(acc, label='Training Accuracy')
    ax1.plot(val_acc, label='Validation Accuracy')
    ax1.legend(loc='lower right')
    ax1.set_ylabel('Accuracy')
    ax1.set_ylim(0, 1.0) #([min(plt.ylim()),1])
    
    ax2.plot(loss, label='Training Loss')
    ax2.plot(val_loss, label='Validation Loss')
    ax2.legend(loc='upper right')
    ax2.set_ylabel('Cross Entropy')
    ax2.set_xlabel('epoch')
    fig.show()
    fig.savefig(outfile)

def plot_confusion_matrix(model, X, y):
    y_pred = model.predict_classes(X, verbose=0)
    plt.figure(figsize=(8, 6))
    sns.heatmap(pd.DataFrame(confusion_matrix(y, y_pred)), annot=True, fmt='d', cmap='YlGnBu', alpha=0.8, vmin=0)


###################################
#### MultiClass Classification ####
class MulticlassModels:
    def __init__(self, X, y, model=0):
        self.X = X
        self.y = y
        self.model=model
    
    def sr_model(self):
        print("Softmax-regression model")
        sr_model = Sequential()
        sr_model.add(Dense(y.shape[1], input_shape=(X.shape[1],), activation='softmax'))
        
        sr_model.compile(Adam(lr=0.1), loss='categorical_crossentropy', metrics=['accuracy'])
        self.model=sr_model
    
    def ann_model(self):
        print("ANN model")
        deep_model = Sequential()
        deep_model.add(Dense(7310*3, input_shape=(X.shape[1],), activation='relu'))
        deep_model.add(Dense(7310, input_shape=(X.shape[1],), activation='relu'))
        deep_model.add(Dense(y.shape[1], activation='softmax'))
        
        deep_model.compile(Adam(lr=0.01), loss='categorical_crossentropy', metrics=['accuracy'])
        self.model=deep_model


################################################
#### Case Study - MultiClass Classification ####
# We will be using the well known Iris dataset.

import pandas as pd
import os
from pathlib import Path
import numpy as np
import matplotlib.pyplot as plt

from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder, MinMaxScaler
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, KFold
from sklearn.metrics import confusion_matrix, classification_report, mean_squared_error, mean_absolute_error, r2_score
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Activation
from tensorflow.keras.optimizers import Adam

############################################
#### Data Visualization & Preprocessing ####
datadir=Path('/cluster/projects/pughlab/projects/cancer_cell_lines/TCGA_genes')
os.chdir(datadir)
df = pd.read_csv(datadir / 'data' / 'Modal_Total_CN_matrix.csv')
df = df.drop([df.columns[0]], axis=1)


## Feature scaling and organizing X and y
X = df.values[:, :-1]
X = X.astype("float")
X[np.isnan(X)] = 2
#ss = StandardScaler()
#X2 = ss.fit_transform(X)
y = pd.get_dummies(df['cancer_type']).values
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

## setup and fit a multi-class logistic regression model
M = MulticlassModels(X, y)
M.sr_model()
hist = M.model.fit(X_train, y_train, epochs=200, verbose=0, validation_split=0.2)
plot_loss_accuracy(hist, os.path.join(datadir, 'models', 'logistic-regression_performance.png'))

M = MulticlassModels(X, y)
M.ann_model()
hist = M.model.fit(X_train, y_train, epochs=30, verbose=0, validation_split=0.2)
plot_loss_accuracy(hist, os.path.join(datadir, 'models', 'ann_performance.png'))


## Performance
acc = hist.history['accuracy']
val_acc = hist.history['val_accuracy']

loss = hist.history['loss']
val_loss = hist.history['val_loss']
print(val_acc)



## Performance metrics
y_pred_class = M.model.predict_classes(X_test, verbose=0)
y_test_class = np.argmax(y_test, axis=1)
print(classification_report(y_test_class, y_pred_class))
plot_confusion_matrix(M.model, X_test, y_test_class)
